# -*- coding: utf-8 -*-
"""Credit card fraud detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18vTDp1QbPMEAMyAqnv629zJTL8yfOniK

DATA COLLECTION AND CLEANING
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install catboost
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import plotly.graph_objects as go
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.figure_factory as ff
from plotly import tools
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)
import gc
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from catboost import CatBoostClassifier
from sklearn import svm
import lightgbm as lgb
from lightgbm import LGBMClassifier
import xgboost as xgb

pd.set_option('display.max_columns', 100)

RFC_METRIC = 'gini'
NUM_ESTIMATORS = 100
NO_JOBS = 4
VALID_SIZE = 0.20
TEST_SIZE = 0.20
NUMBER_KFOLDS = 5
RANDOM_STATE = 2018

MAX_ROUNDS = 1000
EARLY_STOP = 50
OPT_ROUNDS = 1000
VERBOSE_EVAL = 50
data_df = pd.read_csv("/content/creditcard.csv", on_bad_lines='skip')

data_df = data_df.apply(pd.to_numeric, errors='coerce')

# Drop rows with NaN values that were created by coercion
data_df = data_df.dropna()

data_df.head()

data_df.describe()

total = data_df.isnull().sum().sort_values(ascending = False)
percent = (data_df.isnull().sum()/data_df.isnull().count()*100).sort_values(ascending = False)
pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()

"""DATA IMBALANCE"""

import plotly.graph_objects as go
import plotly.offline as py
py.init_notebook_mode(connected=True)

temp = data_df["Class"].value_counts()
df = pd.DataFrame({'Class': temp.index,'values': temp.values})

trace = go.Bar(
    x = df['Class'],y = df['values'],
    name="Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)",
    marker=dict(color="blue"),
    text=df['values']
)
data = [trace]
layout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',
          xaxis = dict(title = 'Class', showticklabels=True),
          yaxis = dict(title = 'Number of transactions'),
          hovermode = 'closest',width=600
         )
fig = dict(data=data, layout=layout) # Defining the fig variable
py.iplot(fig, filename='class') # Use py.iplot to plot the figure
plt.show()

"""DATA EXPLORATION"""

class_0 = data_df.loc[data_df['Class'] == 0]["Time"]
class_1 = data_df.loc[data_df['Class'] == 1]["Time"]

hist_data = [class_0, class_1]
group_labels = ['Not Fraud', 'Fraud']

fig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)
fig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))
iplot(fig, filename='dist_only')
plt.show()

data_df['Hour'] = data_df['Time'].apply(lambda x: np.floor(x / 3600))

tmp = data_df.groupby(['Hour', 'Class'])['Amount'].aggregate(['min', 'max', 'count', 'sum', 'mean', 'median', 'var']).reset_index()
df = pd.DataFrame(tmp)
df.columns = ['Hour', 'Class', 'Min', 'Max', 'Transactions', 'Sum', 'Mean', 'Median', 'Var']
df.head()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Sum", data=df.loc[df.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Sum", data=df.loc[df.Class==1], color="red")
plt.suptitle("Total Amount")
plt.show();

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Transactions", data=df.loc[df.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Transactions", data=df.loc[df.Class==1], color="red")
plt.suptitle("Total Number of Transactions")
plt.show();

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Mean", data=df.loc[df.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Mean", data=df.loc[df.Class==1], color="red")
plt.suptitle("Average Amount of Transactions")
plt.show();

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Max", data=df.loc[df.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Max", data=df.loc[df.Class==1], color="red")
plt.suptitle("Maximum Amount of Transactions")
plt.show();

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Median", data=df.loc[df.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Median", data=df.loc[df.Class==1], color="red")
plt.suptitle("Median Amount of Transactions")
plt.show();

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Min", data=df.loc[df.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Min", data=df.loc[df.Class==1], color="red")
plt.suptitle("Minimum Amount of Transactions")
plt.show();

"""AMOUNT OF TRANSACTION"""

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="Class", y="Amount", hue="Class",data=data_df, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="Class", y="Amount", hue="Class",data=data_df, palette="PRGn",showfliers=False)
plt.show();

tmp = data_df[['Amount','Class']].copy()
class_0 = tmp.loc[tmp['Class'] == 0]['Amount']  # Define class_0 here
class_1 = tmp.loc[tmp['Class'] == 1]['Amount']
class_0.describe()  # Now you can use class_0

class_1.describe()

fraud = data_df.loc[data_df['Class'] == 1]

trace = go.Scatter(
    x = fraud['Time'],y = fraud['Amount'],
    name="Amount",
     marker=dict(
                color='rgb(238,23,11)',
                line=dict(
                    color='red',
                    width=1),
                opacity=0.5,
            ),
    text= fraud['Amount'],
    mode = "markers"
)
data = [trace]
layout = dict(title = 'Amount of fraudulent transactions',
          xaxis = dict(title = 'Time [s]', showticklabels=True),
          yaxis = dict(title = 'Amount'),
          hovermode='closest'
         )
fig = dict(data=data, layout=layout)
iplot(fig, filename='fraud-amount')

"""FEATURES CORRELATION"""

plt.figure(figsize = (14,14))
plt.title('Credit Card Transactions features correlation plot (Pearson)')
corr = data_df.corr()
sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap="Reds")
plt.show()

import seaborn as sns
s = sns.lmplot(x='V20', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})
s = sns.lmplot(x='V7', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})
plt.show()

"""FEATURES DENSITY PLOT"""

var = data_df.columns.values

i = 0
t0 = data_df.loc[data_df['Class'] == 0]
t1 = data_df.loc[data_df['Class'] == 1]

sns.set_style('whitegrid')
plt.figure()
fig, ax = plt.subplots(8,4,figsize=(16,28))

for feature in var:
    i += 1
    plt.subplot(8,4,i)
    sns.kdeplot(t0[feature], bw=0.5,label="Class = 0")
    sns.kdeplot(t1[feature], bw=0.5,label="Class = 1")
    plt.xlabel(feature, fontsize=12)
    locs, labels = plt.xticks()
    plt.tick_params(axis='both', which='major', labelsize=12)
plt.show();

target = 'Class'
predictors = [
    'Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
    'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',
    'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'
]

"""TRAIN TEST SPLIT"""

train_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True )
train_df, valid_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(
    n_jobs=NO_JOBS,
    random_state=RANDOM_STATE,
    criterion=RFC_METRIC,
    n_estimators=NUM_ESTIMATORS,
    verbose=False
)

"""ACCURACY"""

from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
import pandas as pd

# Assuming train_df and valid_df are your training and validation DataFrames
# Also assuming that predictors is your list of feature names and target is the name of your target column

# Preprocess the data to handle missing values
imputer = SimpleImputer(strategy='mean')  # You can choose other strategies like 'median', 'most_frequent', etc.

# Fit the imputer on the training data and transform both training and validation data
train_df[predictors] = imputer.fit_transform(train_df[predictors])
valid_df[predictors] = imputer.transform(valid_df[predictors])

# Instantiate and fit the RandomForestClassifier
clf = RandomForestClassifier(
    n_jobs=NO_JOBS,
    random_state=RANDOM_STATE,
    criterion=RFC_METRIC,
    n_estimators=NUM_ESTIMATORS,
    verbose=False
)

clf.fit(train_df[predictors], train_df[target].values)

# Get predictions
preds = clf.predict_proba(valid_df[predictors])[:, 1]

# Calculate ROC AUC score
roc_auc = roc_auc_score(valid_df[target].values, preds)

print(f"ROC AUC Score: {roc_auc}")

from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
import pandas as pd

# Assuming train_df and valid_df are your training and validation DataFrames
# Also assuming that predictors is your list of feature names and target is the name of your target column

# Define your variables (replace these with actual values)
NO_JOBS = -1
RANDOM_STATE = 42
RFC_METRIC = 'gini'
NUM_ESTIMATORS = 100

# Preprocess the data to handle missing values in predictors
imputer = SimpleImputer(strategy='mean')  # You can choose other strategies like 'median', 'most_frequent', etc.

# Fit the imputer on the training data and transform both training and validation data
train_df[predictors] = imputer.fit_transform(train_df[predictors])
valid_df[predictors] = imputer.transform(valid_df[predictors])

# Handle missing values in the target column
# You can either drop rows with missing target values or fill them with a specific value
train_df = train_df.dropna(subset=[target])

# Instantiate and fit the RandomForestClassifier
clf = RandomForestClassifier(
    n_jobs=NO_JOBS,
    random_state=RANDOM_STATE,
    criterion=RFC_METRIC,
    n_estimators=NUM_ESTIMATORS,
    verbose=False
)

clf.fit(train_df[predictors], train_df[target].values)

# Ensure the classifier was fitted correctly
if hasattr(clf, 'n_classes_'):
    # Get predictions
    preds = clf.predict_proba(valid_df[predictors])[:, 1]

    # Calculate ROC AUC score
    roc_auc = roc_auc_score(valid_df[target].values, preds)

    print(f"ROC AUC Score: {roc_auc}")
else:
    print("The classifier was not fitted correctly.")

tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})
tmp = tmp.sort_values(by='Feature importance',ascending=False)
plt.figure(figsize = (7,4))
plt.title('Features importance',fontsize=14)
s = sns.barplot(x='Feature',y='Feature importance',data=tmp)
s.set_xticklabels(s.get_xticklabels(),rotation=90)
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Sample data creation for demonstration (use your actual valid_df, target, and preds)
# Replace these lines with your actual data
import numpy as np

# Simulating data for example purposes
np.random.seed(0)
valid_df = pd.DataFrame({
    'target': np.random.choice([0, 1], size=10000, p=[0.995, 0.005])
})
preds = np.random.choice([0, 1], size=10000, p=[0.995, 0.005])

# Calculate the confusion matrix
cm = pd.crosstab(valid_df['target'].values, preds, rownames=['Actual'], colnames=['Predicted'])

# Plot the confusion matrix
fig, ax1 = plt.subplots(ncols=1, figsize=(5, 5))

sns.heatmap(cm,
            xticklabels=['Not Fraud', 'Fraud'],
            yticklabels=['Not Fraud', 'Fraud'],
            annot=True,
            fmt='d',  # This ensures that the annotations are displayed as integers
            ax=ax1,
            linewidths=.2,
            linecolor="Darkblue",
            cmap="Blues")

plt.title('Confusion Matrix', fontsize=14)
plt.show()

# Check column names in valid_df
print(valid_df.columns)

# Check the predictors list
print(predictors)

# Ensure predictors are a subset of valid_df columns
missing_columns = [col for col in predictors if col not in valid_df.columns]
if missing_columns:
    print(f"Missing columns in valid_df: {missing_columns}")
else:
    # If columns match, proceed with prediction
    preds = clf.predict_proba(valid_df[predictors])[:, 1]
    roc_auc = roc_auc_score(valid_df[target].values, preds)
    print(f"ROC AUC Score: {roc_auc}")

"""EXPLAINING RECALL CONCEPT WITH A MUCH SMALLER EXAMPLE"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import recall_score
import pandas as pd

# Example data for demonstration (replace with actual data)
valid_df = pd.DataFrame({
    'Time': [0, 1, 2, 3, 4, 5],
    'V1': [0.1, -0.2, 0.3, 0.4, -0.5, 0.6],
    'V2': [0.4, 0.5, -0.6, 0.7, -0.8, 0.9],
    'V3': [0.7, -0.8, 0.9, 1.0, -1.1, 1.2],
    'V4': [1.0, -1.1, 1.2, 1.3, -1.4, 1.5],
    'V5': [1.3, -1.4, 1.5, 1.6, -1.7, 1.8],
    'V6': [1.6, -1.7, 1.8, 1.9, -2.0, 2.1],
    'V7': [1.9, -2.0, 2.1, 2.2, -2.3, 2.4],
    'V8': [2.2, -2.3, 2.4, 2.5, -2.6, 2.7],
    'V9': [2.5, -2.6, 2.7, 2.8, -2.9, 3.0],
    'V10': [2.8, -2.9, 3.0, 3.1, -3.2, 3.3],
    'V11': [3.1, -3.2, 3.3, 3.4, -3.5, 3.6],
    'V12': [3.4, -3.5, 3.6, 3.7, -3.8, 3.9],
    'V13': [3.7, -3.8, 3.9, 4.0, -4.1, 4.2],
    'V14': [4.0, -4.1, 4.2, 4.3, -4.4, 4.5],
    'V15': [4.3, -4.4, 4.5, 4.6, -4.7, 4.8],
    'V16': [4.6, -4.7, 4.8, 4.9, -5.0, 5.1],
    'V17': [4.9, -5.0, 5.1, 5.2, -5.3, 5.4],
    'V18': [5.2, -5.3, 5.4, 5.5, -5.6, 5.7],
    'V19': [5.5, -5.6, 5.7, 5.8, -5.9, 6.0],
    'V20': [5.8, -5.9, 6.0, 6.1, -6.2, 6.3],
    'V21': [6.1, -6.2, 6.3, 6.4, -6.5, 6.6],
    'V22': [6.4, -6.5, 6.6, 6.7, -6.8, 6.9],
    'V23': [6.7, -6.8, 6.9, 7.0, -7.1, 7.2],
    'V24': [7.0, -7.1, 7.2, 7.3, -7.4, 7.5],
    'V25': [7.3, -7.4, 7.5, 7.6, -7.7, 7.8],
    'V26': [7.6, -7.7, 7.8, 7.9, -8.0, 8.1],
    'V27': [7.9, -8.0, 8.1, 8.2, -8.3, 8.4],
    'V28': [8.2, -8.3, 8.4, 8.5, -8.6, 8.7],
    'Amount': [100, 200, 300, 400, 500, 600],
    'target': [0, 1, 0, 0, 1, 0]
})

predictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',
              'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',
              'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27',
              'V28', 'Amount']

target = 'target'

# Train a RandomForestClassifier (example model, replace with your actual model and training data)
clf = RandomForestClassifier()
clf.fit(valid_df[predictors], valid_df[target])

# Predict the probabilities
pred_probs = clf.predict_proba(valid_df[predictors])[:, 1]

# Binarize the predictions based on a threshold (e.g., 0.5)
pred_labels = (pred_probs > 0.5).astype(int)

# Calculate recall
recall = recall_score(valid_df[target].values, pred_labels)
print(f"Recall: {recall}")

# Calculate the confusion matrix
cm = pd.crosstab(valid_df[target].values, pred_labels, rownames=['Actual'], colnames=['Predicted'])

# Plot the confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt

fig, ax1 = plt.subplots(ncols=1, figsize=(5, 5))

sns.heatmap(cm,
            xticklabels=['Not Fraud', 'Fraud'],
            yticklabels=['Not Fraud', 'Fraud'],
            annot=True,
            fmt='d',
            ax=ax1,
            linewidths=.2,
            linecolor="Darkblue",
            cmap="Blues")

plt.title('Confusion Matrix', fontsize=14)
plt.show()